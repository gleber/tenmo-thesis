\newpage
\chapter{Implementation}\label{sec:impl}

To show that the Provenance-Enhanced Distributed Systems Tracing model (see~\cref{sec:pedst-model}) and architecture (see \cref{sec:arch}) solves the use case this thesis is focused on (see~\cref{sec:use-case}) they have been implemented as Tenmo framework\footnote{The code attached to the thesis and available at \url{https://github.com/gleber/tenmo} repository.}. In this chapter we present overview of Tenmo framework and discuss selected details of how it is implemented. Finally we discuss recommended usage patterns around production of Tenmo traces and how such traces can be consumed via queries.

\Cref{sec:results} describes the practical use of Tenmo framework in a couple of software systems.

\section{Overview}

The Tenmo framework implements the PEDST model and follows the PEDST architecture. The implementation has grown organically during the implementations of instrumentation in the software (see \cref{sec:results}), hence little focus has been placed at reusability and cross-language support. At the same time the neither the model, the architecture, nor the current implementation are in any way restrictive -- cross-language support with use of technologies like Protocol Buffers is possible and desired.

Tenmo consists of the following elements:
%
\begin{itemize}
	\item A PostgreSQL-based implementation of data storage.
	\item Go and C++ logging library to enable structured logging as per the logging protocol.
	\item A Python ingestion pipeline which ingests individual structured logging streams.
	\item A Python processor, which listens to notifications coming from the PostgreSQL storage and constructs global data based on it, and stores it in a database.
	\item GraphViz visualization of query results of the global data.\footnote{Diagrams representing Tenmo traces in this thesis are generated by this tool.}
\end{itemize}

Overall our solution has similarities with the LogProv approach~\cite{logprov-wang2016}, where individual data operations are logged as tuples into an provenance events ingestion pipeline. As briefly discussed in \cref{sec:sota}, LogProv uses ElasticSearch to store the tuples, and the graph representation of provenance is recovered from the tuples at query time. LogProv does not deal with failures, since a data processing pipeline abstracts individual failures away. Tenmo, on other hand, has to deal with failures, hence Tenmo is using more granular event-based logging (e.g. separates execution beginning and end events).

\section{Selected details}

In this section, selected details of the implementation will be discussed to give the reader a feel of Tenmo and how it can be used.

\subsection{Data definitions}\label{sec:data-definitions}

In the current implementation global and logging data models are not implemented as separate artifacts. They exist in 3 mirror implementations in Python, Go and PostgreSQL schema. These exactly follows the models defined in~\fullref{sec:global-tuples}, with few additional fields necessary to represent the model in a given language.

For example execution in the global model is defined in Go as:
%
\begin{minted}[]{go}
type Execution struct {
	ExecutionId ExecutionId;
	ParentId ExecutionId;
	CreatorId ExecutionId;
	ProcessId ProcessId;
	Description string;
}
\end{minted}

Here, identifier types are aliases of \texttt{string} type:
%
\begin{minted}[]{go}
type ExecutionId string
type IncarnationId string
type ProcessId string
type EntityId string
type OperationId string
\end{minted}

For example execution begin event in the logging model is defined in Go as:
%
\begin{minted}[]{go}
type eventExecutionBeginsJson struct {
	EventUlid ulid.ULID `json:"event_ulid"`;
	Timestamp time.Time `json:"timestamp"`;
	ExecutionId string `json:"execution_id"`;
	ParentId string `json:"parent_id,omitempty"`;
	CreatorId string `json:"creator_id,omitempty"`;
	ProcessId string `json:"process_id,omitempty"`;
	Description string `json:"description,omitempty"`;
}
\end{minted}

Here, all of the fields are JSON-serializable to allow this structure be stored as a \texttt{payload jsonb} field of the Tenmo storage system (see~\cref{sec:storage} for details).

\subsection{Logging library}\label{sec:logging-library}

Tenmo provides a client-side library, written in Go, and it’s typical execution tracing looks like:
%
\begin{minted}{go}
    parentExecutionId := /* Retrieve parent identifier, often from a Context object. */;
    // Register an execution begin, so it is sent to the Tenmo ingestion pipeline
    executionId, ender := tenmo.ExecutionRegistration(tenmo.Execution{
        tenmo.ExecIdRand("worker-step"),
        parentExecutionId,
        tenmo.None,
        tenmo.None,
        "worker step"})
    // Make sure that the execution is ended at the end of this scope.
    defer ender()
    // Perform own work.
    PerformOwnWork();
    // Pass own execution ID to a child goroutine.
    go PerformChildWork(executionId);
\end{minted}

Here, an execution is registered using an identifier of a parent execution. In Go language, identifier of the current execution is often stored in \texttt{Context} object, which is a typical way to propagate context information around projects. It so happens that Kubernetes project has not yet adopted this pattern, hence Tenmo logging library does not depend on it. Own execution identifier needs to be passed down to any subroutines which are performing any work to be traced or any operations to be tracked.

Go language pattern of using \texttt{defer} for finalizers is suitable here for any executions whose logical lifetime corresponds to a code scope.\footnote{For example a C++ implementation of the library would make use of the RAII pattern, to track the lifetime of executions for most cases.} In case of long-running operations -- as in the intent-based actuation use case -- current execution identifier should be stored in a long-living object.

Next, below, presented is an example of how the read and write operations are recorded. It is worth noting that the incarnation identifier is meant to be recoverable from the object managed by the system. Here, a simple implementation would combine the object's primary key in a database and the value of its last-modified-timestamp column.
%
\begin{minted}{go}
    var executionId tenmo.ExecutionId = /* Fetch execution identifier, usually from Context */;
    MyResource myResource = fetchResource(/* A given resource identifier */);
    entityId := myResource.GetTenmoEntityId();
    currentResourceIncarnationId := myResource.GetTenmoIncarnationId();
    tenmo.OperationRegistration(tenmo.Operation{
		executionId,
		tenmo.OpRead,
		entityId,
		currentResourceIncarnationId,
		"resource Foo", "resource Foo before update"})
    myResource.UpdateAndSave();
    updatedResourceIncarnationId := myResource.GetTenmoIncarnationId();
	tenmo.OperationRegistration(tenmo.Operation{
		executionId,
		tenmo.OpWrite,
		entityId,
		updatedResourceIncarnationId,
		"resource Foo", "resource Foo after update"})
\end{minted}

Here, a process fetches and updates a resource. The process has an existing execution identified by \texttt{executionId} associated with it. The execution records its two operations of reading and writing of the resource.

Below is an example of how annotations can be attached to executions:
%
% GP: To jest tak naprawdę nie zaimplementowane w bibliotece, ale istnieje w schemacie bazy danych...
%
\begin{minted}{go}
    annotationPayload := /* a JSON-serializable struct stored in an annotation */;
    var executionId tenmo.ExecutionId = /* Fetch execution identifier, usually from Context */;
    annotationJson, _ := json.Marshal(annotationPayload)
    tenmo.AnnotateExecution(tenmo.Annotation{executionId, annotationJson})
\end{minted}

Specifically, in current implementation annotations are essentially JSON objects attached to an execution at a specific timestamp. By default the system timestamp is taken.

\begin{comment}
GP: One nie są zaimplementowane, więc pomijam.
To explicitly register existence of an entity, one can:
%
\begin{minted}{cpp}
auto tracer = tenmo::GetDefaultTracer();
tenmo::Entity entity("entity-identifier" , "Description");
tracer.RegisterEntity(&entity);
\end{minted}

Here,.....

To explicitly register existence of a process, one can:
%
\begin{minted}{cpp}
auto tracer = tenmo::GetDefaultTracer();
tenmo::Process process("process-identifier" , "Description");
tracer.RegisterProcess(&process);
\end{minted}

TODO more examples.
\end{comment}

Registration of other objects in the PEDST model using the Tenmo Go logging library is analogous.

In the current implementation of the Tenmo Go logging library, events are directly inserted to the \texttt{events} table of the Tenmo PostgreSQL-based storage.

\subsection{Storage}\label{sec:storage}

The Tenmo implements a basic non-distributed storage based on a single PostgreSQL database. The database consists of two main sections:
%
\begin{itemize}
	\item an event stream table, and
	\item a set of tables capturing the observed universe.
\end{itemize}

\begin{figure}[t]
    \centering
\digraph[scale=0.7]{storageDiagram24}{
  graph [
    rankdir="RL"
    labeljust="l"
    nodesep="0.18"
    ranksep="0.46"
    fontsize="11"
    ration="compress"
  ];
  node [
    fontsize="11"
    shape="plaintext"
  ];
  edge [
    arrowsize="0.8"
  ];
  "entities" [
   label=<
    <TABLE BORDER="0" CELLBORDER="1" CELLSPACING="0">
      <TR><TD COLSPAN="3" ALIGN="CENTER"><B>entities</B></TD></TR>
      <TR><TD PORT="entity_id" COLSPAN="3" ALIGN="LEFT"><TABLE BORDER="0" CELLSPACING="0" ALIGN="LEFT"><TR ALIGN="LEFT"><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="15" HEIGHT="16">PK</TD><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="66" HEIGHT="16">entity_id</TD></TR></TABLE></TD></TR>
      <TR><TD PORT="stored_at" COLSPAN="3" ALIGN="LEFT"><TABLE BORDER="0" CELLSPACING="0" ALIGN="LEFT"><TR ALIGN="LEFT"><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="15" HEIGHT="16"></TD><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="66" HEIGHT="16">stored_at</TD></TR></TABLE></TD></TR>
      <TR><TD PORT="description" COLSPAN="3" ALIGN="LEFT"><TABLE BORDER="0" CELLSPACING="0" ALIGN="LEFT"><TR ALIGN="LEFT"><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="15" HEIGHT="16"></TD><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="66" HEIGHT="16">description</TD></TR></TABLE></TD></TR>
    </TABLE>>
    tooltip="entities"
  ];
  "executions" [
   label=<
    <TABLE BORDER="0" CELLBORDER="1" CELLSPACING="0">
      <TR><TD COLSPAN="3" ALIGN="CENTER"><B>executions</B></TD></TR>
      <TR><TD PORT="execution_id" COLSPAN="3" ALIGN="LEFT"><TABLE BORDER="0" CELLSPACING="0" ALIGN="LEFT"><TR ALIGN="LEFT"><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="15" HEIGHT="16">PK</TD><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="104" HEIGHT="16">execution_id</TD></TR></TABLE></TD></TR>
      <TR><TD PORT="stored_at" COLSPAN="3" ALIGN="LEFT"><TABLE BORDER="0" CELLSPACING="0" ALIGN="LEFT"><TR ALIGN="LEFT"><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="15" HEIGHT="16"></TD><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="104" HEIGHT="16">stored_at</TD></TR></TABLE></TD></TR>
      <TR><TD PORT="begin_timestamp" COLSPAN="3" ALIGN="LEFT"><TABLE BORDER="0" CELLSPACING="0" ALIGN="LEFT"><TR ALIGN="LEFT"><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="15" HEIGHT="16"></TD><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="104" HEIGHT="16">begin_timestamp</TD></TR></TABLE></TD></TR>
      <TR><TD PORT="parent_id" COLSPAN="3" ALIGN="LEFT"><TABLE BORDER="0" CELLSPACING="0" ALIGN="LEFT"><TR ALIGN="LEFT"><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="15" HEIGHT="16">FK</TD><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="104" HEIGHT="16">parent_id</TD></TR></TABLE></TD></TR>
      <TR><TD PORT="creator_id" COLSPAN="3" ALIGN="LEFT"><TABLE BORDER="0" CELLSPACING="0" ALIGN="LEFT"><TR ALIGN="LEFT"><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="15" HEIGHT="16">FK</TD><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="104" HEIGHT="16">creator_id</TD></TR></TABLE></TD></TR>
      <TR><TD PORT="process_id" COLSPAN="3" ALIGN="LEFT"><TABLE BORDER="0" CELLSPACING="0" ALIGN="LEFT"><TR ALIGN="LEFT"><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="15" HEIGHT="16">FK</TD><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="104" HEIGHT="16">process_id</TD></TR></TABLE></TD></TR>
      <TR><TD PORT="description" COLSPAN="3" ALIGN="LEFT"><TABLE BORDER="0" CELLSPACING="0" ALIGN="LEFT"><TR ALIGN="LEFT"><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="15" HEIGHT="16"></TD><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="104" HEIGHT="16">description</TD></TR></TABLE></TD></TR>
      <TR><TD PORT="end_timestamp" COLSPAN="3" ALIGN="LEFT"><TABLE BORDER="0" CELLSPACING="0" ALIGN="LEFT"><TR ALIGN="LEFT"><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="15" HEIGHT="16"></TD><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="104" HEIGHT="16">end_timestamp</TD></TR></TABLE></TD></TR>
    </TABLE>>
    tooltip="executions"
  ];

  "incarnations" [
   label=<
    <TABLE BORDER="0" CELLBORDER="1" CELLSPACING="0">
      <TR><TD COLSPAN="3" ALIGN="CENTER"><B>incarnations</B></TD></TR>
      <TR><TD PORT="incarnation_id" COLSPAN="3" ALIGN="LEFT"><TABLE BORDER="0" CELLSPACING="0" ALIGN="LEFT"><TR ALIGN="LEFT"><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="15" HEIGHT="16">PK</TD><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="84" HEIGHT="16">incarnation_id</TD></TR></TABLE></TD></TR>
      <TR><TD PORT="stored_at" COLSPAN="3" ALIGN="LEFT"><TABLE BORDER="0" CELLSPACING="0" ALIGN="LEFT"><TR ALIGN="LEFT"><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="15" HEIGHT="16"></TD><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="84" HEIGHT="16">stored_at</TD></TR></TABLE></TD></TR>
      <TR><TD PORT="entity_id" COLSPAN="3" ALIGN="LEFT"><TABLE BORDER="0" CELLSPACING="0" ALIGN="LEFT"><TR ALIGN="LEFT"><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="15" HEIGHT="16">FK</TD><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="84" HEIGHT="16">entity_id</TD></TR></TABLE></TD></TR>
      <TR><TD PORT="creator_id" COLSPAN="3" ALIGN="LEFT"><TABLE BORDER="0" CELLSPACING="0" ALIGN="LEFT"><TR ALIGN="LEFT"><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="15" HEIGHT="16">FK</TD><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="84" HEIGHT="16">creator_id</TD></TR></TABLE></TD></TR>
      <TR><TD PORT="description" COLSPAN="3" ALIGN="LEFT"><TABLE BORDER="0" CELLSPACING="0" ALIGN="LEFT"><TR ALIGN="LEFT"><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="15" HEIGHT="16"></TD><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="84" HEIGHT="16">description</TD></TR></TABLE></TD></TR>
    </TABLE>>
    tooltip="incarnations"
  ];
  "operations" [
   label=<
    <TABLE BORDER="0" CELLBORDER="1" CELLSPACING="0">
      <TR><TD COLSPAN="3" ALIGN="CENTER"><B>operations</B></TD></TR>
      <TR><TD PORT="operation_id" COLSPAN="3" ALIGN="LEFT"><TABLE BORDER="0" CELLSPACING="0" ALIGN="LEFT"><TR ALIGN="LEFT"><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="15" HEIGHT="16">PK</TD><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="140" HEIGHT="16">operation_id</TD></TR></TABLE></TD></TR>
      <TR><TD PORT="stored_at" COLSPAN="3" ALIGN="LEFT"><TABLE BORDER="0" CELLSPACING="0" ALIGN="LEFT"><TR ALIGN="LEFT"><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="15" HEIGHT="16"></TD><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="140" HEIGHT="16">stored_at</TD></TR></TABLE></TD></TR>
      <TR><TD PORT="ts" COLSPAN="3" ALIGN="LEFT"><TABLE BORDER="0" CELLSPACING="0" ALIGN="LEFT"><TR ALIGN="LEFT"><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="15" HEIGHT="16"></TD><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="140" HEIGHT="16">ts</TD></TR></TABLE></TD></TR>
      <TR><TD PORT="execution_id" COLSPAN="3" ALIGN="LEFT"><TABLE BORDER="0" CELLSPACING="0" ALIGN="LEFT"><TR ALIGN="LEFT"><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="15" HEIGHT="16">FK</TD><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="140" HEIGHT="16">execution_id</TD></TR></TABLE></TD></TR>
      <TR><TD PORT="op_type" COLSPAN="3" ALIGN="LEFT"><TABLE BORDER="0" CELLSPACING="0" ALIGN="LEFT"><TR ALIGN="LEFT"><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="15" HEIGHT="16"></TD><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="140" HEIGHT="16">op_type</TD></TR></TABLE></TD></TR>
      <TR><TD PORT="entity_id" COLSPAN="3" ALIGN="LEFT"><TABLE BORDER="0" CELLSPACING="0" ALIGN="LEFT"><TR ALIGN="LEFT"><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="15" HEIGHT="16">FK</TD><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="140" HEIGHT="16">entity_id</TD></TR></TABLE></TD></TR>
      <TR><TD PORT="incarnation_id" COLSPAN="3" ALIGN="LEFT"><TABLE BORDER="0" CELLSPACING="0" ALIGN="LEFT"><TR ALIGN="LEFT"><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="15" HEIGHT="16">FK</TD><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="140" HEIGHT="16">incarnation_id</TD></TR></TABLE></TD></TR>
      <TR><TD PORT="entity_description" COLSPAN="3" ALIGN="LEFT"><TABLE BORDER="0" CELLSPACING="0" ALIGN="LEFT"><TR ALIGN="LEFT"><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="15" HEIGHT="16"></TD><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="140" HEIGHT="16">entity_description</TD></TR></TABLE></TD></TR>
      <TR><TD PORT="incarnation_description" COLSPAN="3" ALIGN="LEFT"><TABLE BORDER="0" CELLSPACING="0" ALIGN="LEFT"><TR ALIGN="LEFT"><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="15" HEIGHT="16"></TD><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="140" HEIGHT="16">incarnation_description</TD></TR></TABLE></TD></TR>
    </TABLE>>
    tooltip="operations"
  ];
  "processes" [
   label=<
    <TABLE BORDER="0" CELLBORDER="1" CELLSPACING="0">
      <TR><TD COLSPAN="3" ALIGN="CENTER"><B>processes</B></TD></TR>
      <TR><TD PORT="process_id" COLSPAN="3" ALIGN="LEFT"><TABLE BORDER="0" CELLSPACING="0" ALIGN="LEFT"><TR ALIGN="LEFT"><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="15" HEIGHT="16">PK</TD><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="66" HEIGHT="16">process_id</TD></TR></TABLE></TD></TR>
      <TR><TD PORT="stored_at" COLSPAN="3" ALIGN="LEFT"><TABLE BORDER="0" CELLSPACING="0" ALIGN="LEFT"><TR ALIGN="LEFT"><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="15" HEIGHT="16"></TD><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="66" HEIGHT="16">stored_at</TD></TR></TABLE></TD></TR>
      <TR><TD PORT="description" COLSPAN="3" ALIGN="LEFT"><TABLE BORDER="0" CELLSPACING="0" ALIGN="LEFT"><TR ALIGN="LEFT"><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="15" HEIGHT="16"></TD><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="66" HEIGHT="16">description</TD></TR></TABLE></TD></TR>
    </TABLE>>
    tooltip="processes"
  ];
  
  "events" [
   label=<
    <TABLE BORDER="0" CELLBORDER="1" CELLSPACING="0">
      <TR><TD COLSPAN="3" ALIGN="CENTER"><B>events</B></TD></TR>
      <TR><TD PORT="ulid" COLSPAN="2" ALIGN="LEFT"><TABLE BORDER="0" CELLSPACING="0" ALIGN="LEFT"><TR ALIGN="LEFT"><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="15" HEIGHT="16">PK</TD><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="66" HEIGHT="16">ulid</TD></TR></TABLE></TD></TR>
      <TR><TD PORT="status" COLSPAN="2" ALIGN="LEFT"><TABLE BORDER="0" CELLSPACING="0" ALIGN="LEFT"><TR ALIGN="LEFT"><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="15" HEIGHT="16"></TD><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="66" HEIGHT="16">status</TD></TR></TABLE></TD></TR>
      <TR><TD PORT="attempts" COLSPAN="2" ALIGN="LEFT"><TABLE BORDER="0" CELLSPACING="0" ALIGN="LEFT"><TR ALIGN="LEFT"><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="15" HEIGHT="16"></TD><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="66" HEIGHT="16">attempts</TD></TR></TABLE></TD></TR>
      <TR><TD PORT="stored_at" COLSPAN="2" ALIGN="LEFT"><TABLE BORDER="0" CELLSPACING="0" ALIGN="LEFT"><TR ALIGN="LEFT"><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="15" HEIGHT="16"></TD><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="66" HEIGHT="16">stored_at</TD></TR></TABLE></TD></TR>
      <TR><TD PORT="modified" COLSPAN="2" ALIGN="LEFT"><TABLE BORDER="0" CELLSPACING="0" ALIGN="LEFT"><TR ALIGN="LEFT"><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="15" HEIGHT="16"></TD><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="66" HEIGHT="16">modified</TD></TR></TABLE></TD></TR>
      <TR><TD PORT="created_at" COLSPAN="2" ALIGN="LEFT"><TABLE BORDER="0" CELLSPACING="0" ALIGN="LEFT"><TR ALIGN="LEFT"><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="15" HEIGHT="16"></TD><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="66" HEIGHT="16">created_at</TD></TR></TABLE></TD></TR>
      <TR><TD PORT="event_type" COLSPAN="2" ALIGN="LEFT"><TABLE BORDER="0" CELLSPACING="0" ALIGN="LEFT"><TR ALIGN="LEFT"><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="15" HEIGHT="16"></TD><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="66" HEIGHT="16">event_type</TD></TR></TABLE></TD></TR>
      <TR><TD PORT="payload" COLSPAN="2" ALIGN="LEFT"><TABLE BORDER="0" CELLSPACING="0" ALIGN="LEFT"><TR ALIGN="LEFT"><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="15" HEIGHT="16"></TD><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="66" HEIGHT="16">payload</TD></TR></TABLE></TD></TR>
    </TABLE>>
    URL="tables/events.html"
    target="_top"
    tooltip="events"
  ];

  "graph" [
   label=<
    <TABLE BORDER="0" CELLBORDER="1" CELLSPACING="0">
      <TR><TD COLSPAN="3" ALIGN="CENTER"><B>graph</B></TD></TR>
      <TR><TD PORT="source" COLSPAN="2" ALIGN="LEFT"><TABLE BORDER="0" CELLSPACING="0" ALIGN="LEFT"><TR ALIGN="LEFT"><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="15" HEIGHT="16">PK</TD><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="66" HEIGHT="16">source</TD></TR></TABLE></TD></TR>
      <TR><TD PORT="verb" COLSPAN="2" ALIGN="LEFT"><TABLE BORDER="0" CELLSPACING="0" ALIGN="LEFT"><TR ALIGN="LEFT"><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="15" HEIGHT="16">PK</TD><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="66" HEIGHT="16">verb</TD></TR></TABLE></TD></TR>
      <TR><TD PORT="target" COLSPAN="2" ALIGN="LEFT"><TABLE BORDER="0" CELLSPACING="0" ALIGN="LEFT"><TR ALIGN="LEFT"><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="15" HEIGHT="16">PK</TD><TD ALIGN="LEFT" FIXEDSIZE="TRUE" WIDTH="66" HEIGHT="16">target</TD></TR></TABLE></TD></TR>
    </TABLE>>
    URL="tables/events.html"
    target="_top"
    tooltip="events"
  ];
    
  "executions":"creator_id":w -> "executions":"execution_id":e [arrowhead=none dir=back arrowtail=crowodot];
  "executions":"parent_id":w -> "executions":"execution_id":e [arrowhead=none dir=back arrowtail=crowodot];
  "executions":"process_id":w -> "processes":"process_id":e [arrowhead=none dir=back arrowtail=crowodot];
  "incarnations":"creator_id":w -> "executions":"execution_id":e [arrowhead=none dir=back arrowtail=crowodot];
  "incarnations":"entity_id":w -> "entities":"entity_id":e [arrowhead=none dir=back arrowtail=crowodot];
  "operations":"entity_id":w -> "entities":"entity_id":e [arrowhead=none dir=back arrowtail=crowodot];
  "operations":"execution_id":w -> "executions":"execution_id":e [arrowhead=none dir=back arrowtail=crowodot];
  "operations":"incarnation_id":w -> "incarnations":"incarnation_id":e [arrowhead=none dir=back arrowtail=crowodot];

  "operations":"incarnation_description":s -> "graph":"source":n [style=invis];

  { rank=same; "operations"; "graph"; };
  { rank=same; "processes"; "events"; };
}
    \caption{Tenmo storage diagram in PostgreSQL.}
    \label{fig:storage-diagram}
\end{figure}
 
Both types of tables follow the model described in \cref{sec:data-model} very accurately as depicted in \cref{fig:storage-diagram}. For example, the definition of \texttt{operations} table is the following:
%%MG: po for example zwykle jest przecinek - zapytałam autoryteta :)
\begin{minted}{sql}
  create table operations (
    operation_id text primary key,
    stored_at timestamptz not null default now(),
    ts timestamptz not null,
    execution_id text references executions (execution_id),
    op_type char(1), -- 'w' or 'r'
    entity_id text not null references entities (entity_id),
    incarnation_id text not null references incarnations (incarnation_id),
    entity_description text default '',
    incarnation_description text default ''
  );
\end{minted}

The event stream table is populated by the ingestion pipeline and appended to this table as is. The set of tables representing the observed universe is populated by the ingestion pipeline based on the event stream table. Events in the stream are marked as processed or not. Processed events are regularly garbage collected. Schema of the tables closely follows the global data model.

The \texttt{graph} table is discussed in \fullref{sec:tenmo-graph-queries-implementation}.

\subsection{Ingestion pipeline}\label{sec:ingestion-pipeline}

The ingestion pipeline receives data from a instrumentation embedded in the traced system, and delivers it to Tenmo's PostgreSQL database in a form of events stored in the \texttt{events} table. The Tenmo proof-of-concept implementation uses two ingestion mechanisms:
%
\begin{itemize}
    \item Tenmo Go logging client library connects directly to the Tenmo PostgreSQL instance and appends them to the \texttt{events} table.
    \item Nix integration (see \cref{sec:nix-implementation} uses a log tailing Python script \mintinline{shell}{nix-jsonlog-to-tenmo.py}, which iterates over JSON-structured log lines produced by Nix binaries provided with \mintinline{shell}{--log-format internal-json} flag, transforms them to Tenmo event schema and inserts them directly to the PostgreSQL instance.
\end{itemize}

The fact that Tenmo Go logging client library inserts events directly into the target events storage system is an implementation detail. In a production-grade implementation of Tenmo it would be powered by a distributed high-performance log ingestion solution like Syslog-ng, Apache Kafka, FluentD, or Loginson~\cite{loginson-vega2017}.

\subsection{Events processor}\label{sec:events-processor}

Events processor is responsible for implementing an \texttt{observe} function, which would consume events from \texttt{events} storage table and populate all other tables in the storage.

Whenever a new event is appended to \texttt{events} table, a NOTIFY event will be sent via a trigger. Tenmo events processor subscribes to the appropriate channel, hence gets notified about a new unprocessed event. The processing logic is as follows (pseudo-code):
%
\begin{minted}{text}
events = fetch_all_unprocessed_events(order_by=ingestion_timestamp)
for event in events:
    with transaction:
        mark_claimed(event)
    with transaction:
        U = load_universe()
        U’, success = observe(event, U)
        if success == true:
            mark_processed(event)
            store_universe(U)
        else:
            mark_unclaimed(event)
\end{minted}

To ensure that events are not double-processes, event processing starts with claiming the event in an atomic database transaction, followed by the event processing logic proper. 

If events are not inserted in their causal order (as per requirements defined in \cref{sec:local-model}), some events might be left unprocessed until a future processing cycle happens. This could lead to a situation when a backlog of unprocessed events grows indefinitely, e.g. in case of lost events. In our use case -- given the distributed nature of the traced systems -- lost events are inevitable due to failures in the network, infrastructure, or system itself. The processor will increase a per-event counter every time it gets claimed. When an event crosses an attempts count threshold, the event is no longer being considered for processing. A garbage collection mechanism for such messages is yet to be implemented.

In our implementation of the observe mechanism, to support granular fidelity of provenance information, we always over-approximate the provenance according to the closure definitions in \fullref{sec:global-tuples} and \fullref{sec:model-extensions}. Each resulting implicit relationship stored in the database with an appropriate mark. This allows us to query the data at a selected provenance fidelity level, by simply varying a filter applied to a given SQL query (see \cref{sec:query-implementation} for more details). The resulting graph is described below.

\subsection{Tenmo graph}\label{sec:tenmo-graph-queries-implementation}

Whenever a new event is processed, all relationships between PEDST concepts are materialized into a triple-structured table \texttt{graph}. The table stores three fields \texttt{source}, \texttt{verb} and \texttt{target}. \Cref{tbl:tenmo-graph-rels} lists the relationships which are being materialized in the \texttt{graph} table.

\begin{table}[ht]
\centering
\begin{tabular}{l|c|r}
\hline
\textbf{Source} &          \textbf{Verb} &          \textbf{Target} \\
\hline
\multicolumn{3}{c}{Operation affecting an incarnation} \\
\hline
\texttt{incarnation\_id} & \texttt{read\_by} &      \texttt{execution\_id} \\
\texttt{execution\_id} &   \texttt{reads} &         \texttt{incarnation\_id} \\
\texttt{execution\_id} &   \texttt{writes} &        \texttt{incarnation\_id} \\
\texttt{incarnation\_id} & \texttt{written\_by} &   \texttt{execution\_id} \\
\hline
\multicolumn{3}{c}{Child execution and its parent} \\
\hline
\texttt{execution\_id} &   \texttt{child\_of} &     \texttt{parent\_id} \\
\texttt{parent\_id} &      \texttt{parent\_of} &    \texttt{execution\_id} \\
\hline
\multicolumn{3}{c}{Executions} \\
\hline
\texttt{execution\_id} &   \texttt{created\_by} &   \texttt{creator\_id} \\
\texttt{creator\_id} &     \texttt{creator\_of} &   \texttt{execution\_id} \\
\hline
\multicolumn{3}{c}{Incarnation and its entity} \\
\hline
\texttt{incarnation\_id} & \texttt{instance\_of} &  \texttt{entity\_id} \\
\texttt{entity\_id} &      \texttt{entity\_of} &    \texttt{incarnation\_id} \\
\hline
\multicolumn{3}{c}{Interaction and its messages} \\
\hline
\texttt{sender} & \texttt{sent\_to} &  \texttt{target} \\
\texttt{target} & \texttt{received\_from} &  \texttt{sender} \\
\hline
\multicolumn{3}{c}{Sub-incarnations and its parent} \\
\hline
\texttt{incarnation\_id} & \texttt{part\_of} &      \texttt{parent\_id} \\
\texttt{parent\_id} &      \texttt{divides\_into} & \texttt{incarnation\_id} \\
\hline
\multicolumn{3}{c}{Adjacent pair of incarnations in an entity} \\
\hline
\texttt{incarnation\_id} & \texttt{after}         & \texttt{incarnation\_id} \\
\texttt{incarnation\_id} & \texttt{before}        & \texttt{incarnation\_id}
\end{tabular}
\caption{Tenmo graph relationships.}
\label{tbl:tenmo-graph-rels}
\end{table}%

Here, all relationships captured in the model (see \cref{fig:storage-diagram}) are denormalized into a triples form. This allows us to treat all relationships in the model as a composite graph\footnote{Current proof-of-concept implementation assumes that identifiers of all objects referenced in the \texttt{graph} table are globally unique.}, and traverse it as such. Each graph defined in the \cref{sec:global-graphs} and each extension defined in \cref{sec:model-extensions} are contained in this structure. This composite structure allows to implement queries as graph queries over a triples data, while selecting which relationships to traverse on per query basis using filters of verbs.

It should be noted that all of the relationships are represented bi-directionally, allowing us to traverse the graph in both directions of a relationship depending on the need.

Moreover this data representation allows to make full use of extensive research in knowledge retrieval in Resource Description Framework (RDF)~\cite{rdf-auer2007dbpedia} and Semantic Web~\cite{rdf-shadbolt2006semantic} to evolve usability of data gathered using Tenmo. Tenmo's triples store using PostgreSQL is a naive implementation of this concept. Future research is necessary to scale it to large use cases. 

Queries for Tenmo implementation over this graph representation are described in \fullref{sec:query-implementation}.

%% NOT IMPLEMENTED. For development, the processor can be operated in a relaxed mode, which will not maintain correctness invariant, when observing events and, instead, will populate the universe tables with maximum partial information it can recover from the events. This may lead to incorrect data being stored in the universe tables and tools to misbehave, when interacting with these tables.

\subsection{Queries}\label{sec:query-implementation}

Given the triple form chosen for storing the PEDST graph for querying, answering queries -- with or without graph extensions -- is a matter of crawling the composite graph, and analysing found paths.

Current proof-of-concept Tenmo implementation uses PostgreSQL's recursive Common Table Expressions (CTE) as a mechanism to crawl a graph. The following building blocks are used to answer queries in PEDST model.

\begin{minted}[fontsize=\scriptsize]{plpgsql}
  CREATE OR REPLACE FUNCTION get_all_paths_from(start text)
  RETURNS TABLE(depth integer, verbs text[], path text[]) AS $$
  BEGIN
  RETURN QUERY
  WITH RECURSIVE search_step(id, link, verb, depth, route, verbs, cycle) AS (
    SELECT r.source, r.target, r.verb, 1,
           ARRAY[r.source],
           ARRAY[r.verb]::text[],
           false
    FROM graph r where r.source=start
    UNION ALL
    SELECT r.source, r.target, r.verb, sp.depth+1,
           sp.route || r.source,
           sp.verbs || r.verb,
           r.source = ANY(route)
    FROM graph r, search_step sp
    WHERE r.source = sp.link AND NOT cycle
  )
  SELECT sp.depth, array_append(sp.verbs, '<end>') AS verbs, sp.route || sp.link AS path
  FROM search_step AS sp
  WHERE NOT cycle
  ORDER BY depth ASC;
\end{minted}

This implementation follows PostgreSQL manual on CTE \cite{PostgreSQL-ctes-15:online}  to implement a search for all paths in the graph. The search starts with the node \texttt{start}, given as an argument to the function (line 1). The non-recursive term (lines 6-10) of the recursive CTE identifies all rows which represent outgoing edges from the selected object identified with \texttt{start}. The recursive term of the CTE (lines 12-17) then traverses the graph, identifies cycles (line 15) and records traversal paths (line 13) and verbs encountered on each path (line 14). The final \texttt{SELECT} statement (lines 19-22) filters out irrelevant paths -- e.g. paths containing cycles -- and finalizes the resulting multiset. The result contains a single path per row, with path lengths information in \texttt{depth} column, all traversed objects on the path in \texttt{path} column and all verbs traversed verbs on the path in \texttt{verbs} column.

All other queries are implemented using the same principle. They are described by examples below.

\subsubsection{``Buggy deployment'' queries}

The use case from \fullref{sec:motivating-ex1} as modelled in \cref{sec:motivating-ex1-pedst} is used below for example.

For example, there are 3900 paths coming from the execution representing the final container running the guestbook application:

\begin{minted}[fontsize=\scriptsize]{psql}
tenmo=> select count(*) from get_all_paths_from('remote-app-container');
 count 
-------
  3900
(1 row)
\end{minted}

Selecting a couple of shortest paths about the execution gives us immediate useful information:
%
\begin{minted}[fontsize=\scriptsize]{psql}
tenmo=> select * from get_all_paths_from('remote-app-container') limit 6;
 depth |           verbs           |                         path                         
-------+---------------------------+------------------------------------------------------
     1 | {child_of,<end>}          | {remote-app-container,ssh-remote-docker-run-1}
     1 | {created_by,<end>}        | {remote-app-container,remote-docker-daemon}
     1 | {reads,<end>}             | {remote-app-container,remote-app-2}
     1 | {reads,<end>}             | {remote-app-container,remote-config-1}
     2 | {reads,written_by,<end>}  | {remote-app-container,remote-config-1,scp1}
     2 | {reads,instance_of,<end>} | {remote-app-container,remote-config-1,remote-config}
(6 rows)
\end{minted}
%
Here, we can see that the application has been launched due to a \texttt{ssh remote -- docker run} command and the docker container running the application has been created by Docker daemon running on the \texttt{remote} host. Next we can observe that the container reads the second incarnation of the app container image on host \texttt{remote} (from \texttt{reads} of \texttt{remote-app-2}). We can see that it also reads a config stored on host \texttt{remote} (from \texttt{reads} of \texttt{remote-config-1}) and that this config is an instance of an entity \texttt{remote-config}.

Let us try doing activity tracing and identify all ancestors of the execution representing the application running in the container after the deployment:
\begin{minted}[fontsize=\scriptsize]{psql}
tenmo=> select * from trace('remote-app-container');
 depth |           obj           
-------+-------------------------
     1 | ssh-remote-docker-run-1
     2 | ssh-remote-3
     3 | deploy-script.sh-run-1
(3 rows)
\end{minted}
%
Here we can see that four ancestors were correctly identified, and we know that the current instance of the application has been started by a \texttt{docker run} command, executed by \texttt{ssh remote}, finally executed by the deployment script.

The \texttt{trace} function is implemented in the following way:
%
\begin{minted}[fontsize=\scriptsize]{plpgsql}
  CREATE OR REPLACE FUNCTION trace(start text)
  RETURNS TABLE(depth integer, obj text) AS $$
  BEGIN
  RETURN QUERY
  select * from get_closure_from_by_verbs(start, ARRAY['child_of']::text[]) as t;
  END;
  $$ LANGUAGE plpgsql;
\end{minted}
%
Here the \texttt{get\_closure\_from\_by\_verbs} function is used with \texttt{child\_of} verbs filter. It means that this function will return a closure of all objects in the graph which are reachable by traversing only the edges marked with the given verb. Please refer to ``Appendix A'' for implementation of the \texttt{get\_closure\_from\_by\_verbs} and other query SQL functions.

If one would like to traverse the graph to find all paths denoting only reads and writes, one could start exploring with:
%
\begin{minted}[fontsize=\scriptsize]{psql}
tenmo=> select count(*) from get_all_paths_from_by_verbs('remote-app-container', '{reads,written_by}');
 count 
-------
    14
(1 row)

tenmo=> select max(depth) from get_all_paths_from_by_verbs('remote-app-container', '{reads,written_by}');
 max 
-----
   9
(1 row)
\end{minted}
%
This tells us that there are 14 distinct paths through the \cref{def:effects-graph} and the longest path through the graph is of length 9.

Let us list just the immediate \texttt{reads} edges from the \texttt{remote-app-container}:
%
\begin{minted}[fontsize=\scriptsize]{psql}
tenmo=> select * from get_all_paths_from_by_verbs('remote-app-container', '{reads}') where depth <= 1;
 depth |     verbs     |                  path                  
-------+---------------+----------------------------------------
     1 | {reads,<end>} | {remote-app-container,remote-app-2}
     1 | {reads,<end>} | {remote-app-container,remote-config-1}
(2 rows)
\end{minted}
%
Here we can see that this execution did read two incarnations. Let us see what are the paths of length up to two only through \texttt{written\_by} -- ``which execution wrote a given incarnation?'' -- and \texttt{reads} -- which incarnations did a given execution read?'' -- edges starting from the remote config incarnation node.
%
\begin{minted}[fontsize=\scriptsize]{psql}
tenmo=> select * from get_all_paths_from_by_verbs('remote-app-container', '{reads,written_by}')
                                                        where depth <= 2;
 depth |          verbs           |                            path                             
-------+--------------------------+-------------------------------------------------------------
     1 | {reads,<end>}            | {remote-app-container,remote-app-2}
     1 | {reads,<end>}            | {remote-app-container,remote-config-1}
     2 | {reads,written_by,<end>} | {remote-app-container,remote-config-1,scp1}
     2 | {reads,written_by,<end>} | {remote-app-container,remote-app-2,ssh-remote-docker-run-1}
(4 rows)
\end{minted}
%
This gives us two paths, one to an execution of \texttt{scp} tool and one to a local config incarnation.

From the above one can deduce that crawling the graph over \texttt{written\_by} and \texttt{reads} allows to identify processes and incarnations which were involved directly or indirectly in the provenance of a given object. Such incarnations form a provenance set. A full closure gives us an indirect provenance set, and restricting ourselves only to paths of lenth up to 2 gives us direct provenance set. Tenmo provides two helper function to compute these sets:
%
\begin{minted}[fontsize=\scriptsize]{psql}
tenmo=> select * from provenance_set('remote-app-container');
 depth |       obj       
-------+-----------------
     1 | remote-app-2
     1 | remote-config-1
(2 rows)

tenmo=> select * from provenance_set_indirect('remote-app-container');
 depth |             obj             
-------+-----------------------------
     1 | remote-app-2
     1 | remote-config-1
     3 | config-1
     3 | remote-app-1
     3 | remote-docker-image-app-1
     5 | registry-docker-image-app-1
     7 | docker-image-app-1
     9 | cwd-1
(8 rows)
\end{minted}

This example does not sport intent-based actuation, hence does not show how Tenmo can be used to query across the actuation barrier. The following examples focus on it.

\subsubsection{``Rollback of source of truth'' queries}

The following examples will use scenario from \fullref{sec:motivating-ex2} as modelled in \cref{sec:motivating-ex2-pedst} is used below for example.

As discussed in \cref{sec:motivating-ex2-pedst}, given the intent-based nature of the deployment service, a traditional tracing is not capable of associating git operations with the application being started:
\begin{minted}[fontsize=\scriptsize]{psql}
tenmo=> select * from trace('start-3');
 depth |        obj        
-------+-------------------
     1 | deployment-3
     2 | deployment-server
(2 rows)
\end{minted}

Use of the indirect provenance closure definition -- it traverses verbs (\texttt{reads}, \texttt{written\_by} -- does not reveal this connection:
%
\begin{minted}[fontsize=\scriptsize]{psql}
tenmo=> select * from provenance_set_indirect('app-3');
 depth |    obj    
-------+-----------
     2 | bin-3
     4 | tmp-bin-3
     6 | tmp-src-3
(3 rows)
\end{minted}

It is necessary to extend types of edges traversed in Tenmo graph to reveal the relationship. There are multiple verbs which give us the desired result.

Extending the graph traversal with verb \texttt{child\_of} to include execution child-to-parent relationships into traversable edges gives us the path:
%
\begin{minted}[fontsize=\scriptsize,breaklines]{psql}
tenmo=> select unnest(path) as obj, unnest(verbs) as verb from (select * from get_all_paths_from_by_verbs('app-3', ARRAY['reads', 'written_by', 'child_of']) where 'src-3' = ANY(path) limit 1) t;
          obj          |    verb    
-----------------------+------------
 app-3                 | written_by
 start-3               | child_of
 deployment-3          | reads
 repo-3                | written_by
 git-commit-and-push-3 | reads
 src-3                 | <end>
(6 rows)
\end{minted}
%
This is an example of usage of ``sub-execution provenance extension''.

The desired result can be also achieved by inclusion of traversal of sub-incarnation relationships:
%
\begin{minted}[fontsize=\scriptsize,breaklines]{psql}
tenmo=> select unnest(path) as obj, unnest(verbs) as verb from (select * from get_all_paths_from_by_verbs('app-3', ARRAY['reads','written_by', 'part_of']) where 'src-3' = ANY(path) limit 1) t;
          obj          |    verb    
-----------------------+------------
 app-3                 | written_by
 start-3               | reads
 bin-3                 | written_by
 copy-3                | reads
 tmp-bin-3             | written_by
 build-3               | reads
 tmp-src-3             | part_of
 tmp-store-3           | written_by
 checkout-3            | reads
 repo-3                | written_by
 git-commit-and-push-3 | reads
 src-3                 | <end>
(12 rows)
\end{minted}
%
This is an example of usage of ``sub-incarnation provenance extension''.

Use of a messaging information along with the activity tracing information in the Tenmo graph gives the result as well:
%
\begin{minted}[fontsize=\scriptsize,breaklines]{psql}
tenmo=> select unnest(path) as obj, unnest(verbs) as verb from (select * from get_all_paths_from_by_verbs('app-3', ARRAY['reads','written_by', 'child_of', 'received_from']) where 'src-3' = ANY(path) AND 'received_from' = ANY(verbs) limit 1) t;
          obj          |     verb      
-----------------------+---------------
 app-3                 | written_by
 start-3               | child_of
 deployment-3          | child_of
 deployment-server     | received_from
 git-commit-and-push-3 | reads
 src-3                 | <end>
(6 rows)
\end{minted}
%
This makes use of the ``sub-execution provenance extension'' extension.

Using these techniques an engineer can easily identify that the bug could have been deployed based on \texttt{src-2} or \texttt{src-3}, which would have not been possible with an OpenTracing model. Presented mechanisms allow to track this relationship in a scalable way and accurate way.

\subsubsection{Summary}

These examples show that Tenmo can answer useful questions about a system under trace using a PEDST model, its graphs and graph extensions. They show that it covers both activity tracing and provenance tracking.

A full list of queries implemented over the Tenmo graph is attached in the appendix A. 

\section{Patterns}\label{sec:patterns}

PEDST is a rather flexible model, which can be used for a large variety of applications, fidelity levels and use cases. In this section we will discuss what is the correct usage of the Tenmo framework, which aspects of the model are optional and how to achieve gradual integration. Also, as specified in~\fullref{sec:logging-model}, a crucial requirement of the logging protocol is ability to recover, extract, reconstruct, or pass along execution, incarnation, and interaction identifiers. This section provides a couple of examples here how typical interactions and control flows in control plane systems can be modeled in PEDST.

\subsection{Execution trees}

Distributed systems tracing is the foundation of our model. As seen in \cref{sec:model-operation}, execution tracing together with operation tracking provide a mechanism to track provenance of incarnations affected by executions in an execution tree. Below we describe two common patterns used to track 

\paragraph{RPC flows}

Recording execution trees is to be done the same way it is done in the OpenTracing model, with the following differences:
%
\begin{itemize}
	\item there is no global trace identifier, hence
	\item the parent needs to pass its identifier to any child it creates, so that
	\item each execution is able to record its parent identifier, unless it is the root execution
	\item each logical process is responsible for recording its own execution, interaction and messages
\end{itemize}

Execution tracing in the Tenmo implementation does not differ much from an OpenTracing-based instrumentation. To correctly implement execution tracing, in an RPC handler, it should use the bracket pattern~\cite{BracketPattern2019Apr} (or in C++ the RAII pattern~\cite{RAII2020May}) to record a units of work, performed in the handler. As an input for the recording, a parent execution identifier should be retrieved from the incoming RPC metadata. Whenever an RPC is sent, it’s own execution identifier should be included in the outgoing RPC metadata. The same instrumentation points as originally proposed in the Dapper tech report are applicable.

Independently of the presented logical work units relationship tracing, RPC interactions between executions should be tracked as incarnations and messages (see~\cref{sec:model-interaction}).

% TODO: Image.

\paragraph{Unix process flows}

\begin{wrapfigure}{r}{0.4\textwidth}
\resizebox{0.4\textwidth}{!}{%
\begin{tikzpicture}[node distance=2cm, yscale=-1]

\node[draw,text width=8cm] at (3, 0) (bash) [process] {bash};
\node at (0, 6) (fileA) [decision] {fileA};
\node at (1, 3) (cat) [process] {cat};
\node at (6, 6) (fileB) [decision] {fileB};

\draw [arrow] (cat) -- (bash.south-|cat.north) node [pos=0.7, fill=white] {parent};

\draw [arrow] (fileA) -- (cat) node [midway, fill=white] {read};
\draw [arrow, dotted] (cat) -- (bash) node [pos=0.4, fill=white] {message};
\draw [arrow] (bash) -- (fileB) node [midway, fill=white] {write};

\end{tikzpicture}
}
\caption{Tracing of \mintinline{bash}{cat fileA > fileB}}
\label{fig:unix-process-flow}
\end{wrapfigure}

Execution tracing for Unix processes (see \cref{fig:unix-process-flow}) is analogous to the RPC handler use case. The same information needs to be passed between parent and child processes, for example, in environmental variables or command line arguments. Analogous recordings should be performed using a bracket pattern.

Data transfer between processes using file descriptors -- e.g. stdin, stdout or named pipes -- can be represented as interactions and messages. This maintains provenance relationship between ``fileA`` and ``fileB`` as per ``message-passing provenance extension'' (see \fullref{sec:model-extensions} for details). 

\paragraph{Control loops}

Most servers include continuously-running control loops in them, be it a socket listening loop or a work queue processing loop. These loops should, usually, be recorded as two levels of executions: one long-running parent execution and a number of short-lived child executions dispatching incoming events (see~\cref{fig:control-loops}). Please note that the dispatch executions are usually creators of the event handler executions, but not their parents.

\begin{figure}[t]
\centering
\begin{tikzpicture}[node distance=2cm, yscale=-1]

\node at (2, 0) (handler1) [process] {request 1};
\node at (7, 0) (handler2) [process] {request 2};

\node[draw,text width=12cm] at (5, 2) (loop) [process] {control loop};

\node at (3, 4) (work1) [process] {work unit 1};
\node at (8, 4) (work2) [process] {work unit 2};

\draw [arrow] (work1) -- (handler1) node [pos=0.8, fill=white] {parent};
\draw [arrow] (work2) -- (handler2) node [pos=0.8, fill=white] {parent};
\draw [arrow, dotted] (work1) -- (loop) node [midway, fill=white] {creator};
\draw [arrow, dotted] (work2) -- (loop) node [midway, fill=white] {creator};

\end{tikzpicture}
\caption{Representation of a typical control loop in in PEDST model.}
\label{fig:control-loops}
\end{figure}

\begin{comment}
BRAK CZASU
\subsection{Entity lifecycle}

TODO: Describe how provenance needs to be tracked with Tenmo:

\begin{itemize}
	\item Whenever an entity is changed (e.g. it’s update time in DB is changed), a new incarnation is created, hence a process responsible for the operation needs to log a write operation with the new incarnation.
\end{itemize}

TODO: Code example.

TODO: Image.
\end{comment}

\subsection{Intent-based actuation}

As seen in \cref{sec:model-operation}, execution tracing together with operation tracking provide a mechanism to track provenance of incarnations affected by executions in an execution tree. 

Intent-based actuation usually consists of two levels of continuously running control loops:
%
\begin{itemize}
	\item user request logic, which modified a mutable object which represents intent;
	\item actuation loop, which gets unsuspended whenever intent changes to update a resource under management to match the intent (i.e. actuation).
\end{itemize}

Each of these loops is handled as such, and they are interfacing with each other via write and read operations, of the intent entity. This approach allows us to handle both synchronous and asynchronous triggering of actuation.

%% TODO: Image\todo{!}

\subsection{Intent-based actuation with difference checks}

Some automation systems with intent-based actuation will have an additional mechanism of performing periodic difference checks between desired intent and current state of the world to ensure that they match. In this case the actuation loop can be triggered not only by incoming user requests, but also by the detected difference.

This means that a difference check should also be recorded as a child execution of the intent actuation loop execution, which would perform a read operation on the underlying managed object.

% TODO: Image.

\subsection{Intent-based actuation with intent and status split}\label{sec:intent-with-split}

In case of a system which has an explicit tracking of status of resources under intent-based actuation (like in Kuberenetes~\cite{k8sObjects2020}), additional entity for status needs to be tracked. Status represents the current state of the managed resource, and it is used both for returning it through an API and for internal pre-actuation checks. For example Kubernetes maintains a generation number for Deployments as part of status and uses it as part of completeness checks. The explicit split is useful for API consumers, since they can check if the generation number observed by a control plane service corresponds to the desired state of that object.

In the presented model, intent and status of the object should be tracked separately as shown on \cref{fig:intent-with-split}. This means that API handler execution would perform a write operation on an intent entity. An execution recording a single actuation cycle would typically perform a read of the current intent incarnation, a read of the current status incarnation, and a write on the intent entity.

\begin{figure}[t]
\centering
\scalebox{.85}{
\begin{tikzpicture}[node distance=2cm, yscale=-1]

\node at (0, 0) (handler1) [process] {request 1};
\node at (5, 0) (handler2) [process] {request 2};

\node at (2, 2) (spec1) [decision] {spec 1};
\node at (7, 2) (spec2) [decision] {spec 2};

\node[draw,text width=12cm] at (5, 4.5) (loop) [process] {loop};

\node at (3, 6.5) (sync1) [process] {sync 1};
\node at (8, 6.5) (sync2) [process] {sync 2};

\node at (0.5, 9) (status0) [decision] {status 0};
\node at (5.5, 9) (status1) [decision] {status 1}; 

\draw [arrow] (handler1) -- (spec1) node [midway, fill=white] {write};
\draw [arrow] (handler2) -- (spec2) node [midway, fill=white] {write};
\draw [arrow, dotted] (status0) -- (sync1) node [midway, fill=white] {read};
\draw [arrow, dotted] (spec1) -- (sync1) node [pos=0.2, fill=white] {read};
\draw [arrow] (sync1) -- (status1) node [midway, fill=white] {write};
\draw [arrow, dotted] (status1) -- (sync2) node [midway, fill=white] {read};
\draw [arrow, dotted] (spec2) -- (sync2) node [pos=0.2, fill=white] {read};

\draw [arrow] (sync1) -- (loop) node [midway, fill=white] {parent};
\draw [arrow] (sync2) -- (loop) node [midway, fill=white] {parent};

%\draw [arrow] (fileA) -- (cat) node [midway, fill=white] {read};
%\draw [arrow, dotted] (cat) -- (bash) node [midway, fill=white] {message};
%\draw [arrow] (bash) -- (fileB) node [midway, fill=white] {write};

\end{tikzpicture}
}
\caption{Representation of an intent-based controller with explicit split in PEDST model.}
\label{fig:intent-with-split}
\end{figure}

\subsection{Process definitions as inputs}\label{sec:process-definitions-as-inputs}

When an execution is recorded, the definition of the procedure, the execution represents, is also a source of provenance information. For example let’s take a build process done by \texttt{make} tool using the following rules stored in file \texttt{Makefile}:

\begin{minted}{makefile}
    a.out:
        gcc a.cc -o a.out
\end{minted}

If we want to maintain highest fidelity level, up until internal logic of `make` tool, the following provenance entries will be recorded:
%
\begin{itemize}[nosep]
	\item An execution which represents ``make'' Unix process lifetime with the following operations recorded
    \begin{itemize}[nosep]
    	\item Read of file ``Makefile'' to load the build graph
    \end{itemize}
	\item An execution representing the build process of ``a.out'' with the following operations recorded
    \begin{itemize}[nosep]
    	\item Read of file ``a.out'' to check if it up to date
    \end{itemize}
	\item An execution representing \mintinline{bash}{gcc a.cc -o a.out} Unix process lifetime with the following operations recorded:
    \begin{itemize}[nosep]
    	\item Read of binary ``gcc'' file and all of its dynamic libraries
    	\item Read of file ``a.cc''
    	\item Write of file ``a.out''
    \end{itemize}
\end{itemize}

This will cause a provenance set for the latest incarnation of the entity ``a.out'' to be $ \{ $ `a.cc`, `gcc`, `libc`, $ \ldots $ , `Makefile`$ \} $ . This is an accurate representation of a provenance relationship between objects in a system, but it might not be practically useful for a user of the system. The fact that the build process has ``gcc'' and ``libc'' as inputs is, in most cases, not interesting for the user of the ``make'` tool, when debugging a build process. Usefulness of ``Makefile'' presence in the provenance set is also situational, if a user is debugging a specific build attempt it is not relevant, but it would be relevant if a user is debugging an evolution of the build process over time.

Both Unix binaries and build definition files -- from the perspective of provenance tracking -- are inputs for the build process and they represent records of process definition of an execution. At the same time most of the time in practice these inputs are not relevant for day-to-day development and operations of HCPS. Including these provenance relations will make data much noisier, since event a typical simple binary has multiple dynamically linked libraries.\footnote{On author's machine ``cat'' has 7, ``gcc'' and ``make'' have 4 each.}

Tenmo framework is not prescriptive here and leaves this up to the integrator to decide if process definitions should be part of provenance tracking in their system for their use case.

\subsection{Sub-incarnations}

It is typical for some processes to produce large bundles of objects, as an output of their processing. Few examples are a build target, which is an archive in any build system and a fully-evaluated target state of a cluster in any declarative cluster deployment tool. The Tenmo protocol does not require the writer of the parent incarnation to also record creation of all of the sub-incarnations, since this approach does not scale well for large build targets. Instead, the sub-incarnation provenance propagation mechanism will ensure that provenance is properly tracked between a parent incarnation and its sub-incarnations.

Let’s take the following `Makefile` rule as an example:

\begin{minted}{makefile}
    archive.zip: foo.txt baz.bin
        zip $@ $^
\end{minted}

In this case, an execution representing the execution of `archive.zip` build rule in directory `/opt/foo` will have two file inputs `/opt/foo/foo.txt` and `/opt/foo/baz.bin` and a single output of `/opt/foo/archive.zip` file. As we know, the resulting archive will contain copies of the input files, which we could record as entities under names `/opt/foo/archive.zip:foo.txt` and `/opt/foo/archive.zip:bar.bin`. A hypothetical Tenmo instrumentation of the `zip` executable should, in this case, record three incarnations:
%
\begin{itemize}[nosep]
	\item `/opt/foo/archive.zip` file incarnation
	\item `/opt/foo/archive.zip:foo.txt` sub-incarnation of the archive incarnation
	\item `/opt/foo/archive.zip:bar.bin` sub-incarnation of the archive incarnation
\end{itemize}

%% TODO IMAGE \todo{!}

Accordingly whenever another process reads a file, out of the archive, they should record a fact of reading of a sub-incarnation representing an archived file they read.
